import json
import os
import pandas as pd
import numpy as np
import xgboost as xgb
import seaborn as sns
import matplotlib.pyplot as plt
import joblib

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

def parse_file(path):
    features = []
    labels = []

    parse_data = {}

    with open(path, 'r', encoding='utf-8-sig') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                data = obj.get("Numbers", [])
                foot_vec = -1
                foot_name = obj.get("Foot_Name", None)
                if 'Right' in foot_name:
                    foot_vec = 0
                elif 'Left' in foot_name:
                    foot_vec = 1

                time = obj.get("Save_Time_Now", None).replace(':', '_').replace('.', '_')

                if time not in parse_data:
                    parse_data[time] = {}
                parse_data[time][foot_vec] = data

            except json.JSONDecodeError:
                continue

    return parse_data

def process_folder(folder_path):
    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            print(f"processing {filename}")

            # å¯é€‰ï¼šè·³è¿‡é .json æ–‡ä»¶
            if not filename.endswith('.json'):
                continue

            try:
                names = filename.split("__")

                left_area = names[0]
                right_area = names[1]
                motion_state = names[2]
                travel_state = names[3]
                trips_count = names[4]

                left_area = int(left_area.split('_')[-1])
                right_area = int(right_area.split('_')[-1])
                trips_count = int(trips_count.split('.')[0])

                file_path = os.path.join(root, filename)
                parse_data = parse_file(file_path)

                yield {
                    'left_area_label': left_area,
                    'right_area_label': right_area,
                    'motion_state_label': motion_state,
                    'travel_state_label': travel_state,
                    'trips_count_label': trips_count,
                    'data': parse_data,
                }
            except Exception as e:
                print(f"Error processing {filename}: {e}")
                continue  # æˆ–è€… raiseï¼Œå–å†³äºä½ æ˜¯å¦å¸Œæœ›ä¸­æ–­

def extract_sample_features(sample):
    def _extract_window_features(channel_list, window_size=500, stride=250):
        arr = np.array(channel_list)   # shape (T, 246)
        T, C = arr.shape

        samples = []

        for start in range(0, T - window_size + 1, stride):
            window = arr[start:start + window_size]   # (200, 246)
            feature_dict = {}

            # é’ˆå¯¹æ¯ä¸ªé€šé“æå–ç‰¹å¾
            for ch in range(C):
                x = window[:, ch]
                dx = np.diff(x)

                # --- åŸºç¡€ç»Ÿè®¡ç‰¹å¾ ---
                feature_dict[f'ch{ch}_mean']  = float(np.mean(x))
                feature_dict[f'ch{ch}_std']   = float(np.std(x))
                feature_dict[f'ch{ch}_median'] = float(np.median(x))
                feature_dict[f'ch{ch}_min']   = float(np.min(x))
                feature_dict[f'ch{ch}_max']   = float(np.max(x))
                # feature_dict[f'ch{ch}_range'] = float(np.max(x) - np.min(x))
                # feature_dict[f'ch{ch}_p25']   = float(np.percentile(x, 25))
                # feature_dict[f'ch{ch}_p75']   = float(np.percentile(x, 75))

                # # --- èƒ½é‡ç‰¹å¾ ---
                # feature_dict[f'ch{ch}_energy'] = float(np.mean(x ** 2))
                # feature_dict[f'ch{ch}_rms']    = float(np.sqrt(np.mean(x ** 2)))

                # # --- å˜åŒ–ç‰¹å¾ ---
                # feature_dict[f'ch{ch}_mad']    = float(np.mean(np.abs(dx)))
                # feature_dict[f'ch{ch}_maxdiff'] = float(np.max(np.abs(dx)))
                # feature_dict[f'ch{ch}_slope_mean'] = float(np.mean(dx))

                # # è¿‡é›¶ç‡ï¼ˆé›¶ç‚¹å˜å·æ¬¡æ•°ï¼‰
                # feature_dict[f'ch{ch}_zcr'] = float(np.sum(np.sign(x[1:]) != np.sign(x[:-1])))

                # # --- é¢‘åŸŸç‰¹å¾ ---
                # fft_vals = np.abs(np.fft.rfft(x))
                # freqs = np.fft.rfftfreq(len(x))

                # # ä¸»é¢‘ä¸ä¸»é¢‘å¹…
                # idx = np.argmax(fft_vals)
                # feature_dict[f'ch{ch}_dom_freq'] = float(freqs[idx])
                # feature_dict[f'ch{ch}_dom_amp']  = float(fft_vals[idx])

                # # è°±ç†µ
                # psd = fft_vals ** 2
                # psd_norm = psd / (np.sum(psd) + 1e-8)
                # feature_dict[f'ch{ch}_spec_entropy'] = float(-np.sum(psd_norm * np.log(psd_norm + 1e-8)))

            samples.append(feature_dict)

        return samples
    
    timestamps = list(sample['data'].keys())
    channel_list = []
    for value in sample['data'].values():
        if 0 in value and 1 in value:
            # channel_list.append(value[0] + value[1])
            channel_list.append(value[1][:246:10])

    features = _extract_window_features(channel_list)
    for feature in features:
        feature['left_area_label'] = sample['left_area_label']
        feature['right_area_label'] = sample['right_area_label']
        feature['motion_state_label'] = sample['motion_state_label']
        feature['travel_state_label'] = sample['travel_state_label']
        feature['trips_count_label'] = sample['trips_count_label']

    return features

def train():
    data_folder = "train_datas"
    feature_dicts = []
    for sample in process_folder(data_folder):
        features = extract_sample_features(sample)
        for feature in features:
            feature_dicts.append(feature)

    df_features = pd.DataFrame(feature_dicts)
    # ================================ è®­ç»ƒæ¨¡å‹ =================================
    label_columns = ['left_area_label']
    all_label_columns = ['left_area_label', 'right_area_label', 'motion_state_label', 'travel_state_label', 'trips_count_label']
    confusion_matrices = []

    for label in label_columns:
        print(f"Processing {label}...")
        
        X = df_features.drop(columns=all_label_columns)  # ç‰¹å¾
        y_raw = df_features[label]  # åŸå§‹æ ‡ç­¾ï¼ˆå¯èƒ½æ˜¯ 0,2,3 ç­‰ï¼‰

        le = LabelEncoder()
        y = le.fit_transform(y_raw)

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.1, random_state=42, stratify=y
        )

        model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
        model.fit(X_train, y_train)

        model_filename = f'{label}_model.joblib'
        encoder_filename = f'{label}_label_encoder.joblib'
        joblib.dump(model, model_filename)
        joblib.dump(le, encoder_filename)  # ä¿å­˜ç¼–ç å™¨ï¼Œç”¨äºæ¨ç†æ—¶è¿˜åŸæ ‡ç­¾

        y_pred = model.predict(X_test)

        cm = confusion_matrix(y_test, y_pred)
        confusion_matrices.append((label, cm))

        print(f"Label mapping for {label}:")
        for idx, cls in enumerate(le.classes_):
            print(f"  Encoded {idx} -> Original {cls}")

    # ================================ ç”»çƒ­åŠ›å›¾ =================================
    for label, cm in confusion_matrices:
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
        plt.title(f'Confusion Matrix for {label}', fontsize=16, pad=20)
        plt.xlabel('Predicted label', fontsize=12)
        plt.ylabel('True label', fontsize=12)
        plt.tight_layout()
        
        filename = f'confusion_matrix_{label}.png'
        plt.savefig(filename, dpi=200, bbox_inches='tight')
        print(f"Saved confusion matrix to {filename}")
        
        plt.close()  # é‡è¦ï¼šå…³é—­å½“å‰ figureï¼Œé˜²æ­¢å†…å­˜æ³„æ¼

def predict(json_path):
    parse_data = parse_file(json_path)

    sample = {
        "data": parse_data,
        "left_area_label": 0,
        "right_area_label": 0,
        "motion_state_label": "",
        "travel_state_label": "",
        "trips_count_label": 0,
    }

    feature_dicts = extract_sample_features(sample)

    if len(feature_dicts) == 0:
        print("âŒ æ— å¯ç”¨ç‰¹å¾ï¼Œæ–‡ä»¶æ•°æ®ä¸è¶³ã€‚")
        return

    df = pd.DataFrame(feature_dicts)
    drop_cols = [
        'left_area_label',
        'right_area_label',
        'motion_state_label',
        'travel_state_label',
        'trips_count_label'
    ]
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')

    all_label_columns = [
        'left_area_label',
        'right_area_label',
        'motion_state_label',
        'travel_state_label',
        'trips_count_label'
    ]

    results = {}

    for label in all_label_columns:
        model_file = f"{label}_model.joblib"
        enc_file   = f"{label}_label_encoder.joblib"

        if not (os.path.exists(model_file) and os.path.exists(enc_file)):
            print(f"âš ï¸ æ¨¡å‹æˆ–ç¼–ç å™¨ç¼ºå¤±: {label}")
            continue

        model = joblib.load(model_file)
        le    = joblib.load(enc_file)

        y_pred = model.predict(df)
        y_label = le.inverse_transform(y_pred)

        counts = pd.Series(y_label).value_counts()

        results[label] = {
            "per_window_predictions": list(y_label),
            "vote_result": counts.idxmax(),
            "vote_distribution": counts.to_dict()
        }

    print("\n================ PREDICTION ================\n")
    for label, data in results.items():
        print(f"ğŸ”µ {label}: {data['vote_result']}")
        print(f"    {data['vote_distribution']}")
        print()

    return results

if __name__ == "__main__":
    # train()
    predict("datas/feet_dataxx/Obj1__left_3__right_1/left_3__right_1__brisk_walking__turn_left__1__5066.json")